{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP Q LEARNING ALGORITHM\n",
    "#We are implementing a Deep Q-Learning (DQN) algorithm to optimize traffic light control at an intersection.\n",
    "#The agent observes the number of vehicles in two directions (East-West and North-South) and the current traffic light phase.\n",
    "#It then selects actions to either switch or maintain the traffic light phase.\n",
    "#The goal is to minimize traffic congestion and waiting times by learning the best sequence of traffic light changes.\n",
    "#The agent learns through interactions with the environment, receiving rewards based on the traffic flow efficiency.\n",
    "#This approach uses deep neural networks to approximate the Q-values for state-action pairs.\n",
    "\n",
    "import numpy as np               # For numerical operations, especially for array manipulation\n",
    "import random                    # For generating random numbers for initial actions or states\n",
    "import tensorflow as tf          # Framework for building and training neural networks\n",
    "from tensorflow import keras     # High-level neural network API\n",
    "from tensorflow.keras import layers  # Used to define different layers in a neural network model\n",
    "from collections import deque     # Data structure to efficiently store and sample from memory\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.1         # Alpha: controls the speed of learning updates\n",
    "discount_factor = 0.9       # Gamma: balances importance of immediate vs. future rewards\n",
    "exploration_rate = 1.0      # Epsilon: controls exploration vs. exploitation in action selection\n",
    "exploration_decay = 0.99    # Decay rate for epsilon to reduce exploration over time\n",
    "num_episodes = 10000        # Total learning episodes\n",
    "max_steps_per_episode = 100 # Max actions per episode\n",
    "batch_size = 32             # Number of experiences used in each training step\n",
    "\n",
    "# Environment parameters\n",
    "num_vehicles_per_lane = 12              # Max number of vehicles per lane (EW or NS)\n",
    "num_traffic_phases = 2                  # Two possible traffic light phases: EW green or NS green\n",
    "total_states = num_vehicles_per_lane * num_vehicles_per_lane * num_traffic_phases  # Total states in the environment\n",
    "num_actions = num_traffic_phases       # Actions: 0 = keep current phase, 1 = switch phase\n",
    "\n",
    "# Neural Network for Deep Q-Learning\n",
    "class DQNAgent:\n",
    "    def __init__(self, total_states, num_actions):\n",
    "        self.total_states = total_states  # Total possible states\n",
    "        self.num_actions = num_actions    # Total possible actions\n",
    "        self.memory = deque(maxlen=2000)   # Replay memory to store past experiences\n",
    "        self.gamma = discount_factor      # Discount factor for future rewards\n",
    "        self.epsilon = exploration_rate   # Exploration rate\n",
    "        self.epsilon_decay = exploration_decay  # Decay of exploration rate\n",
    "        self.epsilon_min = 0.01           # Minimum exploration rate\n",
    "        self.model = self.create_model()  # Main model for Q-learning\n",
    "        self.target_model = self.create_model()  # Target model (for stability)\n",
    "        self.update_target_model()        # Initialize the target model with the main model\n",
    "\n",
    "    # Create the neural network model\n",
    "    def create_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(24, activation='relu', input_dim=self.total_states))  # First hidden layer\n",
    "        model.add(layers.Dense(24, activation='relu'))  # Second hidden layer\n",
    "        model.add(layers.Dense(self.num_actions, activation='linear'))  # Output layer (Q-values for each action)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')  # Compile model with mean squared error loss\n",
    "        return model\n",
    "\n",
    "    # Choose an action based on the current state (with exploration vs exploitation)\n",
    "    def act(self, state):\n",
    "        state = np.reshape(state, [1, self.total_states])  # Reshape state for model input\n",
    "        if np.random.rand() <= self.epsilon:  # Exploration: random action\n",
    "            return random.choice(range(self.num_actions))\n",
    "        q_values = self.model.predict(state)  # Exploitation: choose action with highest Q-value\n",
    "        return np.argmax(q_values[0])  # Return the index of the action with the max Q-value\n",
    "\n",
    "    # Store experiences in memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))  # Add experience to memory\n",
    "\n",
    "    # Train the model using a batch of experiences sampled from memory\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:  # If there's not enough memory, return\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)  # Sample a batch of experiences\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward  # Initial target is the immediate reward\n",
    "            if not done:  # If episode is not done, calculate future reward\n",
    "                target += self.gamma * np.amax(self.target_model.predict(next_state)[0])  # Q-value for next state\n",
    "            target_f = self.model.predict(state)  # Get current Q-values for the state\n",
    "            target_f[0][action] = target  # Update Q-value for the action taken\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)  # Train the model\n",
    "        if self.epsilon > self.epsilon_min:  # Decay exploration rate\n",
    "            self.epsilon *= self.epsilon_decay  # Reduce exploration over time\n",
    "\n",
    "    # Update the target model to match the main model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())  # Copy weights from the main model to the target model\n",
    "\n",
    "# Initialize the DQN agent\n",
    "agent = DQNAgent(total_states, num_actions)  # Create agent with state and action space sizes\n",
    "\n",
    "# Simulate the environment and train the agent\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment for the episode (random number of vehicles in each lane)\n",
    "    vehicles_ew, vehicles_ns = random.randint(0, num_vehicles_per_lane), random.randint(0, num_vehicles_per_lane)\n",
    "    traffic_phase = random.choice([0, 1])  # Random initial traffic phase (0 for NS green, 1 for EW green)\n",
    "    state = get_state_index(vehicles_ew, vehicles_ns, traffic_phase)  # Convert state to index for neural network\n",
    "\n",
    "    # Simulate the environment over multiple steps (episode)\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose an action using the DQN agent\n",
    "        action = agent.act(np.array([state]))  # Convert state to array format for input to the model\n",
    "\n",
    "        # Update environment state based on the chosen action\n",
    "        prev_vehicles_ew, prev_vehicles_ns = vehicles_ew, vehicles_ns  # Store previous vehicle counts\n",
    "        # Apply traffic simulation logic here (vehicle arrival/departure based on traffic phase)\n",
    "        next_traffic_phase = action if action == 1 else traffic_phase  # Change phase if action is to switch\n",
    "        next_state = get_state_index(vehicles_ew, vehicles_ns, next_traffic_phase)  # Get next state index\n",
    "\n",
    "        # Calculate the reward based on state transition\n",
    "        reward = calculate_reward(vehicles_ew, vehicles_ns, prev_vehicles_ew, prev_vehicles_ns, traffic_phase, action)\n",
    "\n",
    "        # Check if the episode is done (e.g., after max steps)\n",
    "        done = (step == max_steps_per_episode - 1)\n",
    "\n",
    "        # Store experience in the agent's memory\n",
    "        agent.remember(np.reshape(state, [1, total_states]), action, reward, np.reshape(next_state, [1, total_states]), done)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        traffic_phase = next_traffic_phase\n",
    "\n",
    "        if done:\n",
    "            agent.update_target_model()  # Update the target model after each episode\n",
    "            break\n",
    "\n",
    "    # Train the model after each episode\n",
    "    agent.replay(batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
